# W-RAG-CHAT-APP

RAG-CHAT-APP is a lightweight, containerized Retrieval-Augmented Generation (RAG) system that uses FAISS for semantic search and integrates with Googleâ€™s Gemini API for content generation. It includes a FastAPI backend with persistent conversation logging using SQLite.

## ğŸ“¦ Features

- âœ… Dockerized API server for easy deployment

- ğŸ” FAISS-based semantic search for relevant context retrieval

- ğŸ§  RAG pipeline with Gemini API integration

- ğŸ’¬ FastAPI endpoint for interactive chat sessions

- ğŸ—ƒï¸ Conversation logging using SQLite

- ğŸš€ Simple shell scripts to manage system lifecycle (start_system.sh, stop_system.sh)

- ğŸŒ Ngrok integration for external access

## ğŸ“ Project Structure
```bash
W-RAG-CHAT-APP/
â”‚
â”œâ”€â”€ data/                            # Persistent volume for database and files
â”‚
â”œâ”€â”€ Dockerfile                       # Docker image specification
â”œâ”€â”€ main.py                          # FastAPI app with RAG logic
â”œâ”€â”€ request.py                       # example request 
â”œâ”€â”€ output_chunks.jsonl              # Text chunks for semantic search
â”œâ”€â”€ rag_prompt_config.jsonl          # Base/system prompt definitions
â”œâ”€â”€ faiss_index.index                # Saved FAISS index (auto-generated)
â”‚
â”œâ”€â”€ start_system.sh                  # Start the app + ngrok
â”œâ”€â”€ stop_system.sh                   # Stop everything
```
## ğŸš€ Getting Started

### Prerequisites

- Docker
- Ngrok (installed and accessible via CLI)
- .jsonl config and chunk files placed in root (as seen above)
- A valid Gemini API Key

### 1. Build the Docker Image
```bash
docker build -t rag-chat .
```

### 2. Run the System
```bash
export GEMINI_API_KEY=your_api_key_here
./start_system.sh
```
### ğŸ’¡ This:
> - Starts the container and exposes it on port `10000`  
> - Mounts the `data/` folder for persistent database storage  
> - Launches an ngrok tunnel in a background `tmux` session

### 3. Stop the System
```bash
./stop_system.sh
```

## ğŸ§  API Usage

### Endpoint
```bash
POST /chat
```
### Request Body
```json
{
          "history": [
            {
                "role": "user",
                "message": "Jak dziaÅ‚a maszyna W?"
            },
            {
                "role": "assistant",
                "message": "Maszyna W jest symulowanym procesorem, dla ktÃ³rego pisze siÄ™ programy w jÄ™zyku asemblera. Program skÅ‚ada siÄ™ z instrukcji (rozkazÃ³w) i danych. KaÅ¼da linia programu zawiera etykietÄ™ (opcjonalnÄ…)..."
            },
            {
                "role": "user",
                "message": "Napisz program w jÄ™zyku W, ktÃ³ry oblicza sumÄ™ liczb od 1 do 10."
            },
  ]
}
```
### Response
```json
{
  "response": "```assembly\n Program oblicza sumÄ™ liczb od 1 do 10\n\n SUMA RPA ; Rezerwacja miejsca na zmiennÄ… SUMA (wynik)\n LICZNIK RST 1 ;\n licznika wartoÅ›ciÄ… 1\n LIMIT RST 10 ; Ustawienie gÃ³rnej granicy pÄ™tli\n\n POCZATEK:\n..."
}
```
## ğŸ§¬ How It Works

### Startup:

- Loads output_chunks.jsonl and rag_prompt_config.jsonl
- Initializes or loads FAISS index
- Sets up SQLite DB
- Request Handling:
- Builds a concatenated prompt using semantic search via FAISS
- Sends this prompt to Gemini API
- Logs the full interaction in the database
- Returns the assistant's response

## ğŸ›  Developer Notes
- Default port: 10000
- DB file: data/conversations.db
- If faiss_index.index does not exist, it is built at runtime